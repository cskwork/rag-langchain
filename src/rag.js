import { CheerioWebBaseLoader } from "@langchain/community/document_loaders/web/cheerio";
import { RecursiveCharacterTextSplitter } from "langchain/text_splitter";
import { StateGraph, START, END } from "@langchain/langgraph";
import { MessagesAnnotation } from "@langchain/langgraph";
import { HumanMessage, AIMessage } from "@langchain/core/messages";
import { EmbeddingsOpenAI } from './wrappers/embeddings-openai.js';
import { chromaWrapper } from './wrappers/chroma-wrapper.js';
import { chatHistoryManager } from './chat-history.js';
import { CONFIG } from './config.js';

/**
 * RAG ì‹œìŠ¤í…œ í´ë˜ìŠ¤ - StateGraph ì‚¬ìš©
 * (RAG System Class with StateGraph)
 */
export class RAGSystem {
  constructor() {
    this.vectorStore = null;
    this.embeddings = null;
    this.graph = null;
    this.conversationalGraph = null;
    this.chromaWrapper = chromaWrapper;
    this.chatHistoryManager = chatHistoryManager;
  }

  /**
   * RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
   * (Initialize RAG system)
   */
  async initialize() {
    try {
      console.log('ğŸš€ Initializing RAG system...');
      
      // OpenRouter API í‚¤ ê²€ì¦
      if (!CONFIG.OPENROUTER.API_KEY || !CONFIG.OPENROUTER.LLM_MODEL) {
        throw new Error('OpenRouter API key and model are required');
      }

      // ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
      this.embeddings = EmbeddingsOpenAI({
        modelName: CONFIG.OPENAI.EMBEDDING_MODEL,
        apiKey: CONFIG.OPENAI.API_KEY
      });

      // ì±„íŒ… íˆìŠ¤í† ë¦¬ ì²´í¬í¬ì¸í„° ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
      try {
        await this.chatHistoryManager.initializeCheckpointer();
      } catch (error) {
        console.warn('âš ï¸ Checkpointer initialization failed, using in-memory storage only:', error.message);
        // ì²´í¬í¬ì¸í„° ì—†ì´ ê³„ì† ì§„í–‰
      }

      console.log('âœ… RAG system initialized successfully');
      
    } catch (error) {
      console.error('âŒ RAG initialization failed:', error.message);
      throw error;
    }
  }

  /**
   * ë¬¸ì„œ ë¡œë“œ ë° ì¸ë±ì‹±
   * (Load and index documents)
   */
  async buildIndex(documentUrl = CONFIG.DEFAULT_DOCUMENT_URL) {
    if (!this.embeddings) {
      throw new Error('RAG system not initialized. Call initialize() first.');
    }

    try {
      console.log('ğŸ“„ Loading documents...');
      
      // 1. ë¬¸ì„œ ë¡œë“œ
      const loader = new CheerioWebBaseLoader(documentUrl);
      const docs = await loader.load();
      console.log(`ğŸ“„ Loaded ${docs.length} document(s)`);

      // 2. í…ìŠ¤íŠ¸ ë¶„í• 
      const textSplitter = new RecursiveCharacterTextSplitter({
        chunkSize: CONFIG.TEXT_SPLITTER.CHUNK_SIZE,
        chunkOverlap: CONFIG.TEXT_SPLITTER.CHUNK_OVERLAP,
        separators: CONFIG.TEXT_SPLITTER.SEPARATORS
      });
      const splitDocs = await textSplitter.splitDocuments(docs);
      console.log(`ğŸ“ Split into ${splitDocs.length} chunks`);

      // 3. Chroma ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ë¬¸ì„œ ì €ì¥
      console.log('ğŸ”— Initializing Chroma vector store...');
      this.vectorStore = await this.chromaWrapper.createVectorStore(
        this.embeddings,
        splitDocs
      );
      console.log('ğŸ’¾ Documents embedded and stored in Chroma vector store');

      // 4. StateGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
      this._createStateGraph();
      
      // 5. ëŒ€í™”í˜• StateGraph ìƒì„± (Conversational StateGraph creation)
      this._createConversationalStateGraph();

      return {
        documentsLoaded: docs.length,
        chunksCreated: splitDocs.length,
        vectorStoreSize: splitDocs.length
      };
    } catch (error) {
      console.error('âŒ Index building failed:', error.message);
      throw error;
    }
  }

  /**
   * StateGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
   * (Create StateGraph workflow)
   */
  _createStateGraph() {
    // ìƒíƒœ ì •ì˜
    const workflow = new StateGraph({
      channels: {
        question: null,
        context: null,
        answer: null
      }
    });

    // ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ
    const retrieveNode = async (state) => {
      console.log(`ğŸ” Retrieving documents for: ${state.question}`);
      
      const docs = await this.vectorStore.similaritySearch(
        state.question,
        CONFIG.RETRIEVAL.TOP_K
      );
      
      const context = docs.map(doc => doc.pageContent).join('\n\n');
      console.log(`ğŸ“š Retrieved ${docs.length} relevant documents`);
      
      return { context };
    };

    // ë‹µë³€ ìƒì„± ë…¸ë“œ
    const generateNode = async (state) => {
      console.log('ğŸ¤– Generating answer...');
      
      const prompt = `Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.

Context: ${state.context}

Question: ${state.question}

Helpful Answer:`;

      const headers = {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${CONFIG.OPENROUTER.API_KEY}`,
      };

      const requestBody = {
        model: CONFIG.OPENROUTER.LLM_MODEL,
        messages: [{ role: 'user', content: prompt }],
        temperature: CONFIG.LLM.TEMPERATURE,
        max_tokens: CONFIG.LLM.MAX_TOKENS,
        top_p: CONFIG.LLM.TOP_P,
      };

      const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`OpenRouter API Error (${response.status}): ${errorText}`);
      }

      const data = await response.json();
      const answer = data.choices[0].message.content;
      
      return { answer };
    };

    // ë…¸ë“œ ì¶”ê°€
    workflow.addNode("retrieve", retrieveNode);
    workflow.addNode("generate", generateNode);

    // ì—£ì§€ ì¶”ê°€
    workflow.addEdge(START, "retrieve");
    workflow.addEdge("retrieve", "generate");
    workflow.addEdge("generate", END);

    // ê·¸ë˜í”„ ì»´íŒŒì¼
    this.graph = workflow.compile();
  }

  /**
   * ëŒ€í™”í˜• StateGraph ì›Œí¬í”Œë¡œìš° ìƒì„± (MessagesAnnotation ì‚¬ìš©)
   * (Create conversational StateGraph workflow using MessagesAnnotation)
   */
  _createConversationalStateGraph() {
    // MessagesAnnotationì„ ì‚¬ìš©í•œ ëŒ€í™”í˜• ì›Œí¬í”Œë¡œìš° ìƒì„±
    const conversationalWorkflow = new StateGraph(MessagesAnnotation);

    // ê²€ìƒ‰ ë…¸ë“œ - ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ë¬¸ì„œ ê²€ìƒ‰
    const retrieveNode = async (state) => {
      const messages = state.messages || [];
      const lastMessage = messages[messages.length - 1];
      
      if (!lastMessage || lastMessage._getType() !== 'human') {
        throw new Error('No user message found');
      }

      const query = lastMessage.content;
      console.log(`ğŸ” Retrieving with conversation context for: "${query}"`);
      
      // ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ê²€ìƒ‰ ìˆ˜í–‰
      const retrievalResult = await this.chatHistoryManager.retrieveWithContext(
        query,
        messages,
        this.vectorStore
      );
      
      console.log(`ğŸ“š Retrieved ${retrievalResult.documents.length} documents with context`);
      
      // ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìƒíƒœì— ì €ì¥
      return {
        messages: messages,
        context: retrievalResult.context,
        searchQuery: retrievalResult.searchQuery,
        originalQuery: retrievalResult.originalQuery
      };
    };

    // ì‘ë‹µ ìƒì„± ë…¸ë“œ - ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ì‘ë‹µ ìƒì„±
    const generateNode = async (state) => {
      const messages = state.messages || [];
      const context = state.context || '';
      const query = state.originalQuery || state.searchQuery;
      
      console.log('ğŸ¤– Generating contextual response...');
      
      // ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ì‘ë‹µ ìƒì„±
      const response = await this.chatHistoryManager.generateContextualResponse(
        query,
        context,
        messages
      );
      
      // AI ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
      const aiMessage = new AIMessage(response);
      
      return {
        messages: [...messages, aiMessage]
      };
    };

    // ë…¸ë“œ ì¶”ê°€
    conversationalWorkflow.addNode("retrieve", retrieveNode);
    conversationalWorkflow.addNode("generate", generateNode);

    // ì—£ì§€ ì¶”ê°€
    conversationalWorkflow.addEdge(START, "retrieve");
    conversationalWorkflow.addEdge("retrieve", "generate");
    conversationalWorkflow.addEdge("generate", END);

    // ëŒ€í™”í˜• ê·¸ë˜í”„ ì»´íŒŒì¼
    this.conversationalGraph = conversationalWorkflow.compile();
    
    console.log('âœ… Conversational StateGraph created successfully');
  }

  /**
   * ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (StateGraph ì‚¬ìš©)
   * (Generate answer for question using StateGraph)
   */
  async generateAnswer(question) {
    if (!this.graph) {
      throw new Error('StateGraph not initialized. Call buildIndex() first.');
    }

    try {
      console.log(`\nâ“ Question: ${question}`);
      
      // StateGraph ì‹¤í–‰
      const result = await this.graph.invoke({
        question: question
      });
      
      console.log(`\nğŸ’¬ Answer: ${result.answer}`);
      return result.answer;
      
    } catch (error) {
      console.error('âŒ Answer generation failed:', error.message);
      throw error;
    }
  }

  /**
   * ëŒ€í™”í˜• ë‹µë³€ ìƒì„± (MessagesAnnotation ì‚¬ìš©)
   * (Generate conversational answer using MessagesAnnotation)
   */
  async generateConversationalAnswer(messages, threadId = 'default') {
    if (!this.conversationalGraph) {
      throw new Error('Conversational StateGraph not initialized. Call buildIndex() first.');
    }

    try {
      console.log(`\nğŸ—£ï¸  Conversational query processing...`);
      
      // ëŒ€í™”í˜• StateGraph ì‹¤í–‰
      const result = await this.conversationalGraph.invoke({
        messages: messages
      });
      
      const aiResponse = result.messages[result.messages.length - 1];
      console.log(`\nğŸ’¬ Conversational Answer: ${aiResponse.content}`);
      
      return {
        answer: aiResponse.content,
        messages: result.messages,
        threadId: threadId
      };
      
    } catch (error) {
      console.error('âŒ Conversational answer generation failed:', error.message);
      throw error;
    }
  }

  /**
   * ëŒ€í™” ì‹œì‘ (ìƒˆë¡œìš´ ëŒ€í™” ì„¸ì…˜ ì‹œì‘)
   * (Start conversation - begin new conversation session)
   */
  async startConversation(initialQuestion, threadId = 'default') {
    try {
      // ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±
      const userMessage = new HumanMessage(initialQuestion);
      const messages = [userMessage];
      
      // ëŒ€í™”í˜• ë‹µë³€ ìƒì„±
      const result = await this.generateConversationalAnswer(messages, threadId);
      
      // ëŒ€í™” ìƒíƒœ ì €ì¥ (ì²´í¬í¬ì¸í„° ì‚¬ìš©)
      const conversationState = {
        messages: result.messages,
        threadId: threadId,
        timestamp: new Date().toISOString()
      };
      
      await this.chatHistoryManager.saveConversationCheckpoint(threadId, conversationState);
      
      return result;
    } catch (error) {
      console.error('âŒ Conversation start failed:', error.message);
      throw error;
    }
  }

  /**
   * ëŒ€í™” ê³„ì† (ê¸°ì¡´ ëŒ€í™”ì— ë©”ì‹œì§€ ì¶”ê°€)
   * (Continue conversation - add message to existing conversation)
   */
  async continueConversation(question, threadId = 'default') {
    try {
      // ê¸°ì¡´ ëŒ€í™” ìƒíƒœ ê°€ì ¸ì˜¤ê¸° (ì²´í¬í¬ì¸í„° ì‚¬ìš©)
      const conversationState = await this.chatHistoryManager.loadConversationCheckpoint(threadId);
      
      // ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
      const userMessage = new HumanMessage(question);
      const messages = [...conversationState.messages, userMessage];
      
      // ëŒ€í™”í˜• ë‹µë³€ ìƒì„±
      const result = await this.generateConversationalAnswer(messages, threadId);
      
      // ëŒ€í™” ìƒíƒœ ì—…ë°ì´íŠ¸ (ì²´í¬í¬ì¸í„° ì‚¬ìš©)
      const updatedConversationState = {
        messages: result.messages,
        threadId: threadId,
        timestamp: new Date().toISOString()
      };
      
      await this.chatHistoryManager.saveConversationCheckpoint(threadId, updatedConversationState);
      
      return result;
    } catch (error) {
      console.error('âŒ Conversation continuation failed:', error.message);
      throw error;
    }
  }

  /**
   * ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± (OpenRouter API ì§ì ‘ í˜¸ì¶œ)
   * (Generate streaming answer using direct OpenRouter API call)
   */
  async *generateAnswerStream(question) {
    if (!this.vectorStore) {
      throw new Error('Vector store not initialized. Call buildIndex() first.');
    }

    try {
      // 1. ë¬¸ì„œ ê²€ìƒ‰ (Document retrieval)
      console.log(`ğŸ” Retrieving documents for: ${question}`);
      const docs = await this.vectorStore.similaritySearch(
        question,
        CONFIG.RETRIEVAL.TOP_K
      );
      
      const context = docs.map(doc => doc.pageContent).join('\n\n');
      console.log(`ğŸ“š Retrieved ${docs.length} relevant documents`);
      
      // 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (Construct prompt)
      const prompt = `Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.

Context: ${context}

Question: ${question}

Helpful Answer:`;

      // 3. ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ (Streaming API call)
      console.log('ğŸ¤– Generating streaming answer...');
      
      const headers = {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${CONFIG.OPENROUTER.API_KEY}`,
      };

      const requestBody = {
        model: CONFIG.OPENROUTER.LLM_MODEL,
        messages: [{ role: 'user', content: prompt }],
        temperature: CONFIG.LLM.TEMPERATURE,
        max_tokens: CONFIG.LLM.MAX_TOKENS,
        top_p: CONFIG.LLM.TOP_P,
        stream: true // ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
      };

      const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
        method: 'POST',
        headers,
        body: JSON.stringify(requestBody),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`OpenRouter API Error (${response.status}): ${errorText}`);
      }

      // 4. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ (Process streaming response)
      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let buffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        buffer += decoder.decode(value, { stream: true });
        const lines = buffer.split('\n');
        buffer = lines.pop() || '';

        for (const line of lines) {
          const trimmed = line.trim();
          if (trimmed === '' || trimmed === 'data: [DONE]') continue;
          
          if (trimmed.startsWith('data: ')) {
            try {
              const data = JSON.parse(trimmed.slice(6));
              if (data.choices && data.choices[0] && data.choices[0].delta && data.choices[0].delta.content) {
                const content = data.choices[0].delta.content;
                yield content;
              }
            } catch (parseError) {
              // íŒŒì‹± ì˜¤ë¥˜ ë¬´ì‹œ (Ignore parsing errors)
              continue;
            }
          }
        }
      }
      
    } catch (error) {
      console.error('âŒ Streaming answer generation failed:', error.message);
      throw error;
    }
  }

  /**
   * ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
   * (Check system status)
   */
  getStatus() {
    return {
      hasEmbeddings: !!this.embeddings,
      hasVectorStore: !!this.vectorStore,
      hasGraph: !!this.graph,
      hasConversationalGraph: !!this.conversationalGraph,
      model: CONFIG.OPENROUTER.LLM_MODEL,
      embeddingModel: CONFIG.OPENAI.EMBEDDING_MODEL,
      chromaStatus: this.chromaWrapper.isInitialized(),
      chatHistoryStatus: {
        hasCheckpointer: !!this.chatHistoryManager.checkpointer,
        conversationCount: this.chatHistoryManager.conversationState.size
      }
    };
  }

  /**
   * Chroma ì»¬ë ‰ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
   * (Get Chroma collection info)
   */
  async getCollectionInfo() {
    return await this.chromaWrapper.getCollectionInfo();
  }

  /**
   * Chroma ì»¬ë ‰ì…˜ ì‚­ì œ
   * (Delete Chroma collection)
   */
  async deleteCollection() {
    await this.chromaWrapper.deleteCollection();
    this.vectorStore = null;
  }

  /**
   * ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
   * (Clean up system resources)
   */
  async cleanup() {
    try {
      console.log('ğŸ§¹ Cleaning up RAG system resources...');
      
      // Chat history manager ì •ë¦¬
      await this.chatHistoryManager.cleanup();
      
      // Chroma ë¦¬ì†ŒìŠ¤ ì •ë¦¬
      await this.chromaWrapper.cleanup();
      
      // ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ ì •ë¦¬
      this.vectorStore = null;
      this.embeddings = null;
      this.graph = null;
      this.conversationalGraph = null;
      
      console.log('âœ… RAG system cleanup completed');
    } catch (error) {
      console.error('âŒ RAG system cleanup failed:', error.message);
      throw error;
    }
  }

  /**
   * ëŒ€í™” ì´ˆê¸°í™”
   * (Reset conversation)
   */
  async resetConversation(threadId = 'default') {
    try {
      await this.chatHistoryManager.deleteConversationCheckpoint(threadId);
      console.log(`ğŸ”„ Conversation reset for thread: ${threadId}`);
      return true;
    } catch (error) {
      console.error('âŒ Conversation reset failed:', error.message);
      throw error;
    }
  }

  /**
   * ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
   * (Get conversation history)
   */
  async getConversationHistory(threadId = 'default') {
    return await this.chatHistoryManager.loadConversationCheckpoint(threadId);
  }

  /**
   * ëª¨ë“  ëŒ€í™” ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
   * (Get all conversation threads)
   */
  async getAllConversationThreads() {
    try {
      const checkpoints = await this.chatHistoryManager.getAllConversationCheckpoints();
      return checkpoints.map(cp => ({
        threadId: cp.threadId,
        messageCount: cp.messageCount,
        timestamp: cp.timestamp,
        lastMessage: cp.messageCount > 0 ? `${cp.messageCount} messages` : 'No messages'
      }));
    } catch (error) {
      console.error('âŒ Failed to get conversation threads:', error.message);
      // í´ë°±: ë©”ëª¨ë¦¬ì—ì„œ ê°€ì ¸ì˜¤ê¸°
      const threads = [];
      for (const [threadId, state] of this.chatHistoryManager.conversationState.entries()) {
        threads.push({
          threadId,
          messageCount: state.messages.length,
          lastMessage: state.messages[state.messages.length - 1]?.content || 'No messages'
        });
      }
      return threads;
    }
  }

  /**
   * ëŒ€í™” ìš”ì•½ ìƒì„±
   * (Generate conversation summary)
   */
  async summarizeConversation(threadId = 'default') {
    const conversationState = this.chatHistoryManager.getConversationHistory(threadId);
    return await this.chatHistoryManager.summarizeConversation(conversationState.messages);
  }
}